\section{Concurrency}
\label{sec:concurrency}

Every program that we have written up to now consisted of one single
thread. The flow of execution started at the beginning of
the \verb+main+ method and continued line-by-line up the last
statement of such method. Method calls were executed orderly,
line-by-line, from the beginning to the end. When methods called other
methods, called methods were executed to the end (or until an
exception was thrown) before the calling method continued its
execution. When an exception was thrown, the flow jumped downwards
until an adequate \verb+catch+ block was found or until the the end of
the method; if the exception found the end of the method, it continued
looking for a \verb+catch+ block on the calling
method, and continued moving up on the stack until it found it 
(maybe up to, and out of, the \verb+main+ method). 

In any case, there was one and only thing to do at any point in
time. Our programs have been \emph{deterministic}. If we knew what
statement had just been executed by the computer, we knew which
statement was going to be executed next (and with which values, etc). 

However, we know that computers can do several things at the same
time. The computer you are working on now is able to show you this
document while accepting input from your keyboard and mouse, 
browsing the internet, 
keeping several services active in the background, and
performing several other tasks at the same time. 
Computers are able to do this by
switching tasks on the microprocessor (some nanoseconds of reading data from the
network, then a bit of reading keyboard input, then painting the
screen, then a bit of network again\ldots), but the switching happens so
fast that it gives the impression to humans that all tasks happen
simultaneously. Additionally, now that multi-processor computers and
multi-core processors have become the norm, computers are able to
actually do more than one task at the same time, one per
microprocessor or core (plus they still
switch tasks fast).

In this section we are going to learn the basics of concurrent
programming, enabling our programs to do more than one thing at the
same time. This is intended to be only a very basic
introduction. Concurrent programming is a very complicated topic,
difficult even
for seasoned programmers\footnote{Concurrent programming is so
  difficult that several programming languages have been 
  created with the explicit goal of easing the creation of concurrent
  programs. Examples range from educational languages like SR to
  production-oriented languages like Erlang, Scala, Go, or
  Clojure (and arguably Java). 
  Scala and Clojure programs run on the Java Virtual Machine and can
  interact easily with Java programs.}. 

\subsection{Why concurrency?}
\label{sec:why-concurrency}

Concurrent programming is harder than simple deterministic
programming, but it bears many important benefits, including: 

\begin{description}
\item[Performance. ] If your task can be separated into many subtasks,
  and each subtask can be done in parallel in different processors or
  machines, the whole task may be completed faster. This is a very
  common approach to perform complicated scientific and engineering
  calculations that take long to complete (e.g.~weather simulations).
\item[Responsiveness. ] Concurrent programs are able ---by
  definition--- to do several things at the same time. This enables 
  concurrent programs to start long calculations in the background
  while still listening to user input, even if the calculation has not
  finished yet. Think of the Java garbage collector, for example; it is
  running at the same time as your programs, looking for unreachable
  memory to release; it does not need your program to end before
  memory can be recovered.
\item[Cleaner design. ] Many applications require different tasks to
  happen at the same time: games require many enemies to move, web
  browsers require several images to be downloaded for visualising a
  web page, videoconference software encodes (outgoing) and decodes
  (incoming) video and sound at the same time, etc. This can be
  programmed as separate tasks and let the operating system (or
  virtual machine) to take care of the switching, which is much
  simpler than taking care of the switching as part of the program. 
\end{description}

\subsection{Basic concepts}
\label{sec:basic-concepts}

\subsubsection{Non-determinism}
\label{sec:non-determinism}

The benefits of concurrent programming come at a cost that we have
hinted at already: determinism. Concurrent programs are not
deterministic, we cannot tell which will be the next statement to be
executed even if we know exactly which one was executed last. 

This is because the machine will keep switching tasks and we \emph{do not
have any control} over that process. The machine will switch tasks
depending on many different factors that we cannot control and this % TODO: search synonym for this or the former "control"
means that our execution flow may be stopped after one or one hundred
statements have been executed, and the context may have changed when
the execution is resumed. Imagine that you are going to take the last
sandwich on the plate but, just before you grab it, it disappears;
this means that the computer stopped you midway through your action
``taking sandwich'', continued
executing another thread (that grabbed the sandwich and ran away), and
then continued executing you\ldots but the context had changed (the
sandwich had disappeared). 

\subsubsection{Processes vs Threads}
\label{sec:processes-vs-threads}

At this point, it is important to distinguish two basic concepts:
processes and threads. A process has its own memory space (stack and
heap) separated from other processes. Switching between processes is
done by the operating system. A Java program runs in its own process. 

Threads (sometimes called lightweight processes) share part of the
same memory space. Each thread has its own stack, but they share
access to the heap and can communicate inside the
program\footnote{Modern operating systems allow for communication
  between processes, but that requires use of special operating system
  calls and is out of the scope of this section.}, e.g.~by accessing
the same variables or objects. 

\emph{Concurrent programming in Java is done by using threads}. The Java
Virtual Machine switches between threads, activating or deactivating
them according to different factors beyond the control of the
programmer. As far as the programmer is concerned, threads run in
parallel at random (and variable) speeds. 

\subsubsection{Atomic operations}
\label{sec:atomic-operations}

An \emph{atomic} operation is an operation that cannot be interrupted
by thread-switching. In Java, every read and every write to a 32-bit
primitive type is atomic (note that this leaves out \verb+double+ and
\verb+long+). In other words, if the computer starts executing the
statement \verb+n = 0+ it cannot change thread until that statement
has finished.

It is important to note ---especially if you have experience with C
or~C++--- that \verb_n++_ is \emph{not} atomic. In Java, that
statement is equivalent to three statements: one read, one increment,
and one write. It is possible that your thread will be interrupted
after the read but before the write. If another thread interacts
with \verb+n+ then, the result is unpredictable and may result in data
loss. 

\subsection{Launching new threads}
\label{sec:launch-new-thre}

Java provides a class called \verb+Thread+ to launch and run new threads. New
threads are defined by using a class that implements interface
\verb+Runnable+. This interface consists of just one simple method: 

\begin{verbatim}
    public void run();
\end{verbatim}

Starting a thread with a new \verb+Runnable+ task is easy. First, we
create the thread, then we start it. Assuming we have a runnable
object called \emph{myRunnable} (that implements 
interface \verb+Runnable+), the code looks like:

\begin{verbatim}
    Thread newThread = new Thread(myRunnable);
    newThread.start();
\end{verbatim}

From this point on, the new thread will be running in parallel with
the main thread of the application (the one started by Java at the
\verb+main+ method) until its \verb+run()+ method
finishes\footnote{There is another way of creating threads: extending
  class Thread instead of implementing Runnable, and then starting it
  directly. This approach is less flexible because Java 
  does not allow for multiple inheritance, so it is rarely used.}. 

\subsection{The problems of concurrency}
\label{sec:problems-concurrency}

We already know that concurrent programs are not deterministic. This
is a big source of bugs. A computer program is quite complex, and it
is difficult to keep track of every single aspect of it. You have
experienced this yourself in past weeks when your programs did not
behave as expected, and you had to debug them: maybe your program was
not updating a variable as you thought it was, or maybe some data was
overwritten and it was not your intention. All of this can happen in
multi-threaded programs, and worse. Look at the following (contrived
but illustrative) code: 

\begin{verbatim}
    private int count = 0;
    // ...
    public int increment() {
       count++;
    }
    public int decrement() {
       count--;
    }
    public int getCount() {
        increment();
        decrement();
        return count;
    }
\end{verbatim}

It seems obvious that any invocation of \verb+getCount()+ method will return
exactly the value of \verb+count+, right?
Unfortunately, this is the kind of ``evident yet false'' behaviour
that is common when multithreaded programs are not studied carefully. 

Let's imagine that we have two threads, T1 and T2, running in
parallel. (Read the following lines carefully, its gets messy). 
\begin{itemize}
\item T1 is executing at the moment, and T2 is ``dormant''.
\item T1 calls method \verb+getCount()+ when \verb+count+ is 0, calls
\verb+increment()+ and increases the value to 1, 
then it is interrupted.
\item T2 resumes its execution, which includes
calling \verb+getCount()+ (\verb+count+ is now 1), and then calls
\verb+increment()+, and increments \verb+count+, that is now 2.
\item T2 is interrupted and T1 resumes execution. It finishes with
\verb+increment()+, returns to \verb+getCount()+ and calls
\verb+decrement()+, decreasing the value of \verb+count+ to 1, then
returns to \verb+getCount()+ and returns 1 (not 0!).
\item T2 continues execution and finishes \verb+getCount()+ returning a
value of 0 (not 1!).
\end{itemize}

If we had more than two threads, the situation could be even more
complicated and unpredictable. For example, a third thread may have called
\verb+increment()+ and \verb+decrement()+ while T1 and T2 had not
finished their execution of \verb+getCount()+.
Keep also in mind that interruptions and thread-switching
could have happened in between the \verb_count++_ and \verb+count--+
statements because they are not atomic despite the fact that they are
just one short line. 

The moral of the story is: concurrent programming is difficult, and
non-deterministic; you have to be very careful or you will get it
wrong.

\subsection{Synchronisation}
\label{sec:synchronization}

Computer programs are lists of instructions to make computers process data in a
reliable (and fast) way. If we want our programs to be reliable even
when using non-deterministic threads we need to make sure threads are
synchronised in some way. The most common way of achieving
synchronisation is by means of \emph{locks}. By acquiring and
releasing locks, we can make sure that our threads do not overlap
with bad results. 

We can think of thread locks as locks on doors: door locks control
access to rooms and thread locks control access to pieces of code. 
When a door is open
(lock free) any person (thread) can open the door and enter (start
executing some code),  
closing the door behind (acquiring the lock). 
Any other persons that tries
to open the door (tries to acquire the lock) 
while another person has locked it
(while another thread has the lock) will be blocked, and will have to wait. As
soon as the person that locked the door opens it and comes out (thread
releases the lock) another person can enter (another thread can
acquire the lock and start executing the code). 

It is important to note that threads do \emph{not} make an
ordered queue in front of a lock like good British citizens. Quite the
opposite: when a lock is released, any thread that was waiting for the
lock may acquire it. You can think of threads as noisy foreigners,
pushing each other to be in front of the door when it opens, ready to
rush in as soon as the person inside gets out. 

In Java, locks are acquired (and released) easily by using one of the
few keywords we have not seen yet: \verb+synchronized+. This keyword
can be used on methods or on fragments of code. 

\subsubsection{Synchronising fragments of code}
\label{sec:synchr-fragm-code}

When used on a fragment of code, it must specify the object from which
to acquire the lock. 
%
Every Java object contains a lock inside, so every Java object can be
used for this purpose. 
%
If the lock is available, it will be acquired and the code
will execute. At the end of the scope, the lock will be released. If
the lock is not available (another thread has it) the thread will
block, waiting for the lock to be available. 

\begin{verbatim}
1    private Object obj = new Object();
2    // ...
3    public int getCount() {
4        synchronized (obj) { 
5            increment();
6            decrement();
7            return count;
8        }
9    }
\end{verbatim}

When T1 enters the \verb+synchronized+ block (lines 4--8), 
it may be interrupted
because the machine switches to~T2. Thanks to the use of locks, 
this will not pose a problem when~T2 executes
\verb+getCount()+ because~T2 will try to acquire the lock of object
\verb+obj+ (line 4), realise it is already acquired by another
thread, and get blocked. When the machine switches back to~T1 at an
undetermined time in the future, it will
continue with its execution until it returns a value
from \verb+getCount()+ (line 7). The lock is released when a thread
reaches the end of a \verb+synchronised+ block, either explicitly (in
the example, by reaching line 8) or implicitly, either by returning a
value (as in this example) or by throwing an exception. 

When the lock on \verb+obj+ is released, T2 (or any other thread
waiting for the same lock, \emph{maybe in other methods of the same object})
will be able to acquire it and continue with their execution. 

Note that we could have used \emph{any} object for locking this block of
code. For example, if \verb+count+ was an \verb+Integer+ instead of an
\verb+int+, we could have used its lock to synchronise the block. We
could have also acquired the lock of \verb+this+ object (the object
where the code is being executed). Note as well
that the object from which we use {\bf the lock must be a field} and 
cannot be a local
variable because every thread has its own stack, so the following code
will not work as intended: 

\begin{verbatim}
    public int getCount() {
        Object obj = new Object(); // Every thread has its own copy 
        synchronized (obj) {       // of object obj (and its lock),
            increment();           // so they do not block each other
            decrement();
            return count;
        }
    }
\end{verbatim}

\subsubsection{Synchronising full methods}
\label{sec:synchr-full}

The second way in which \verb+synchronized+ can be used is to
synchronise full methods, as in this example: 

\begin{verbatim}
    public synchronized int getCount() {
        increment();
        decrement();
        return count;
    }
\end{verbatim}

When it is used in this way, the thread trying to execute the
synchronised method will acquire the lock of \verb+this+ object. In a
way, this is similar to using \verb+synchronized(this)+ on the full
code of the method, but the
effect is a bit different. 

\begin{itemize}
\item Synchronising the whole method has two
benefits: it is simpler (less lines of code) and clearer (the
\verb+synchronized+ will appear in the documentation of your class,
making it explicit to other programmers).
\item Synchronising a fragment has
two benefits: it allows you to synchronise only part of the method
(finer-grained synchronisation) 
and it can use different objects for locks, not just \verb+this+. 
\end{itemize}

Both aspects may be important for making sure that your program does
not block, depending on the application (see
Section~\ref{sec:why-not-make}).

Note that \verb+synchronized+ does \emph{not} make your block of statements
\emph{atomic}. Your thread can still be interrupted an arbitrary
number of times while it is executing a
synchronised block. The only thing that \verb+synchronized+ will do
for you is making sure that other threads do not enter the same block
of statements until the current thread has finished with it (either by
executing all statements or by throwing an exception) and
released the lock. 

\subsubsection*{A note on terminology}
\label{sec:note-terminology}

Locks are sometimes called \emph{mutex}, because they provide \emph{mut}ually
\emph{ex}clusive access to a resource. \emph{Semaphores} are a special type
of lock that allows several threads to gain access to a
resource. Semaphores can be represented as an integer number, and
every thread that gets access to the resource decreases it. When it
gets to zero, the next thread will need to wait until the semaphore
raises again. A lock can be viewed as a semaphore of value 1. 

\subsubsection{Why not make everything synchronised?}
\label{sec:why-not-make}

Synchronising access to resources prevents harmful overlapping of
threads using the same resource, but we cannot make every single
method in our program \verb+synchronized+. First of all,
synchronisation comes at a performance cost. Checking, acquiring, and
releasing locks takes time. Careless use of locks all over our code
will make it run slower. However, this is not our main concern. 

The most important problem of careless synchronisation is that our
program may block. If thread A has acquired lock L1 and is waiting for
lock L2, while thread B has acquired lock L2 and is
waiting for L1, our program cannot continue. This is
called a \emph{deadlock}, and is one of the most common problems with
concurrent programs (the other being race conditions leading
to harmful overlapping of threads accessing the same resource). 
%
Remember that we said that thread locks are like door locks? Imagine
that Andrew stops before the door to let Bob pass first, and the same
time Bob also stops before the door to let Andrew pass first; as each
of them is waiting for the other to act, none of them do: this is a
deadlock.  

There is no easy way to avoid deadlocks in our code: we must be really
careful in our design. Careful design will prevent deadlocks and also 
other less common but equally bad situations: starvation and livelock. 

\emph{Starvation} occurs when one or more of our threads never execute
and/or have access to some resource they need. This may happen for
example if one thread acquires a lock but never releases it; threads
waiting for that lock will remain ``dormant'' forever.
%
A \emph{livelock} is similar to a deadlock in the sense that two or
more threads are blocking each other, but the difference is that the
program is still running, only not making any progress. Imagine that
Andrew and Bob meet in the middle of the corridor; Andrew moves to the
left to let Bob pass, and Bob moves to the right for the same reason:
they are blocked again; then Andrew moves to the right and Bob moves to
the left, blocking each other again; and this continues forever. 

Finally, a \emph{race condition} happens when the outcome of our
program is not reliable and depends on the specific order in which the
threads execute: sometimes it may give the right result, sometimes it
may lose data, sometimes it may block. This is always the result of a
poor understanding of the interaction of the threads and/or poor
synchronisation. It is extremely difficult to debug a program with
race conditions, as they may manifest only with some operating
systems, and/or some versions of the Java Virtual Machine, 
and/or in the presence of some specific data input, and/or
some other apparently harmless factor in the environment. May you
never find yourself debugging a race condition the day before a
deadline. 

\subsubsection{Sleeping, waiting, and notifying}
\label{sec:sleep-wait-notify}

Sometimes a lock is not what we need to synchronise our threads; 
sometimes we want one of your threads to wait until one condition is
true (i.e.~set as true by another thread). For example, the thread
that takes our mail should not read from the inbox until some other
thread has left any mail on it. The block of code that comes after
such a condition is sometimes called a \emph{guarded block}, because
it is guarded/protected by the condition. We could implement guarded
blocks with a simple \verb+while+ loop, as shown below:

\begin{verbatim}
    while (!conditionMet) { 
        // empty loop -- no code
    }
    // Guarded block comes here...
\end{verbatim}

The code above is really wasteful. The computer will spend a lot of
CPU time doing a lot of empty loops. It would be
better to let the computer make a short pause and then continue with
the execution. We can do this by using the static method
\verb+sleep()+ and providing the length of the pause in milliseconds. 

\begin{verbatim}
    while (!conditionMet) {
        try {
            Thread.sleep(1000); // sleep 1000ms = 1 second
        } catch (InterruptedException ex) {
            // Nothing to do in this case, just sleep less...
        }
    }  
    // Guarded block comes here...
\end{verbatim}

(Methods that put a thread on hold, like \verb+Thread.sleep()+, can be
interrupted. If that happens, an \verb+InterruptedException+ will be
thrown. This is a checked exception and, therefore, must be caught).

This is better than the former version, but it still checks every
single second. Maybe the condition will take a long time before it
evaluates to \verb+true+. Is there any way to tell the thread to wait
\emph{indefinitely} until the time comes to wake up again? 

As it happens, there is. An object can call its own method
\verb+wait()+ to interrupt 
the execution of a thread at a given point. The thread will remain
stopped until notified by another thread (using method
\verb+notifyAll()+\footnote{There is also a method notify(). There are
clear differences between notify() and notifyAll(), but their
discussion goes well beyond the scope of this section.}). 

A thread calling \verb+wait()+ or \verb+notifyAll()+ must have the
lock on the object, otherwise Java will throw an
\verb+IllegalMonitorStateException+. The easiest way to acquire the
monitor is by synchronising the method, as in the following code: 

\begin{verbatim}
01    public synchronized void doSomething() {
02        while (!conditionMet) {
03            try {
04                wait(); // sleep until notified
05            } catch (InterruptedException ex) {
06                // Nothing to do in this case, just wait less...
07            }
08        }  
09        // Guarded block comes here...
10    }
11    // ...
12    public synchronized void anotherMethod() {
13        // ...
14        notifyAll();
15        // ...
16    }
\end{verbatim}

When a thread executes \verb+wait()+ (line~4), it releases the lock it had
acquired. Other thread may acquire it then and enter the same
\verb+synchronized+ block of code (or others), and maybe get blocked
at the same \verb+wait()+ call. The lock can also be acquired by another
thread to enter a block of code where it calls
\verb+notifyAll()+ (line~14)). At
this point, all the threads that were waiting on the same object are
woken up ---but they cannot continue executing because the lock is
still with the thread that executed the notification. After this
thread releases the lock (line~16), one of the awaiting threads will re-acquire
it (without the need to re-enter the code, it is already there) and
continue its execution. Note that this may or may not have a real
effect, depending on whether the condition has already been met (line~2); if it
is not, the thread will \verb+wait()+ again (line~4). 

\subsection{Immutable objects}
\label{sec:immutable-objects}

An immutable object is an object whose state cannot be changed after
creation. 

Use of immutable objects is a good strategy for creating simpler and
more reliable code. This is especially true in the case of
multi-threaded programs, where the complexity of interactions of
different parts of the program increases because of the complexity of
interaction between different threads. Since immutable objects cannot
change state, they cannot be corrupted by thread interference and
cannot be observed in an inconsistent state.

The simplest strategy to create immutable objects is declaring all the
class' fields as \verb+final+. However, this is not enough in the general
case. In order to ensure that an object is immutable, the following
rules must be followed: 

\begin{itemize}
\item Make all fields not only \verb+final+, but \verb+private+.
\item Do not provide ``setter'' methods, methods that modify fields
  (impossible since they are final) or objects referred to by fields
  (possible, even if the pointer is \verb+final+).
\item Do not allow subclasses to override methods. The easiest way to
  achieve this is by declaring the class as \verb+final+.
\item If instance fields point to mutable objects (e.g.~a String),
  do not share the reference to those objects. 
  \begin{itemize}
  \item Never store references to objects passed in the constructor,
    make copies and point to those copies.
  \item Never provide references to the objects pointed to by your
    instance fields. Make copies and return references to the copies. 
  \end{itemize}
\end{itemize}

The latter point is commonly referred to as \emph{making defensive
  copies} and is another good strategy in multi-threaded environments,
even if immutable objects are not used. 
%
Since it can be generally assumed that other threads will make
changes to the objects passed to them, it is always a good idea to
pass copies of our own objects instead of references to the objects
themselves. Otherwise, other threads may make changes to the objects
with unpredictable effects on other threads. 

Making defensive copies at construction time or in response to method
calls incurs in a certain overhead, but this rarely constitutes a
bottleneck. The performance cost is generally compensated by the
simplicity gained by the program. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "d17"
%%% End:
